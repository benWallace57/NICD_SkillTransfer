{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Label classification\n",
    "\n",
    "#### Topic Modelling on the Reuters Dataset. \n",
    "\n",
    "Binary classification is where an input can be classified in to 2 categories\n",
    "\n",
    "Multi-class classification is where an input can be classified to any ONE of many categories\n",
    "\n",
    "Multi-label classification is where an input can be classified to ANY NUMBER of many categories \n",
    "\n",
    "In this notebook we'll explore multi-label classification within the field of topic modelling. In our case, stating the topics associated with a reuters news article. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs\n",
    "- nltk is the natural language toolkit, where the reuters dataset is stored\n",
    "- the nltk corpus contains information about the reuters dataset\n",
    "- torch (pytorch) libraries contain helpful tools for deep learning including the dataloader and optimiser\n",
    "- the model and tokeniser are loaded from the transformers library. \n",
    "- label_ranking_average_precision_score is an evaluation tool\n",
    "-  the counter class is helpful for complex counting over iterables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import reuters\n",
    "\n",
    "import torch.utils.data\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification , AutoTokenizer\n",
    "\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /Users/Ben/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"reuters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 90 categories which the documents can be labeled by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_documents = reuters.fileids()\n",
    "reuters_categories = reuters.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns information about the reuters dataset, including the test:train split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_info(documents):\n",
    "\n",
    "    \"\"\"Information about Reuters dataset, such as number of training and test documents, and categories\"\"\"\n",
    "    train_docs = list(filter(lambda doc: doc.startswith(\"train\"), documents))\n",
    "    test_docs = list(filter(lambda doc: doc.startswith(\"test\"), documents))\n",
    "    # 10788 documents, 7769 for training and 3019 for test\n",
    "    print(str(len(documents)) + \" documents\")\n",
    "    print(str(len(train_docs)) + \" total train documents\")\n",
    "    print(str(len(test_docs)) + \" total test documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10788 documents\n",
      "7769 total train documents\n",
      "3019 total test documents\n"
     ]
    }
   ],
   "source": [
    "dataset_info(reuters_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns the number of documents which are labeleld by each of the 90 categories. As this dataset is multi-label the sum of these figures will be more than the number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def documents_per_category(categories):\n",
    "    \"\"\"Return the number of documents per category\"\"\"\n",
    "    def get_category_length_tuple(cat):\n",
    "        return (len(reuters.fileids(cat)), cat)\n",
    "    return [get_category_length_tuple(cat) for cat in categories]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2369, 'acq'),\n",
       " (58, 'alum'),\n",
       " (51, 'barley'),\n",
       " (105, 'bop'),\n",
       " (68, 'carcass'),\n",
       " (2, 'castor-oil'),\n",
       " (73, 'cocoa'),\n",
       " (6, 'coconut'),\n",
       " (7, 'coconut-oil'),\n",
       " (139, 'coffee'),\n",
       " (65, 'copper'),\n",
       " (3, 'copra-cake'),\n",
       " (237, 'corn'),\n",
       " (59, 'cotton'),\n",
       " (3, 'cotton-oil'),\n",
       " (97, 'cpi'),\n",
       " (4, 'cpu'),\n",
       " (578, 'crude'),\n",
       " (3, 'dfl'),\n",
       " (175, 'dlr'),\n",
       " (14, 'dmk'),\n",
       " (3964, 'earn'),\n",
       " (23, 'fuel'),\n",
       " (54, 'gas'),\n",
       " (136, 'gnp'),\n",
       " (124, 'gold'),\n",
       " (582, 'grain'),\n",
       " (9, 'groundnut'),\n",
       " (2, 'groundnut-oil'),\n",
       " (19, 'heat'),\n",
       " (22, 'hog'),\n",
       " (20, 'housing'),\n",
       " (16, 'income'),\n",
       " (6, 'instal-debt'),\n",
       " (478, 'interest'),\n",
       " (53, 'ipi'),\n",
       " (54, 'iron-steel'),\n",
       " (5, 'jet'),\n",
       " (67, 'jobs'),\n",
       " (8, 'l-cattle'),\n",
       " (29, 'lead'),\n",
       " (15, 'lei'),\n",
       " (2, 'lin-oil'),\n",
       " (99, 'livestock'),\n",
       " (16, 'lumber'),\n",
       " (49, 'meal-feed'),\n",
       " (717, 'money-fx'),\n",
       " (174, 'money-supply'),\n",
       " (6, 'naphtha'),\n",
       " (105, 'nat-gas'),\n",
       " (9, 'nickel'),\n",
       " (3, 'nkr'),\n",
       " (4, 'nzdlr'),\n",
       " (14, 'oat'),\n",
       " (171, 'oilseed'),\n",
       " (27, 'orange'),\n",
       " (3, 'palladium'),\n",
       " (40, 'palm-oil'),\n",
       " (3, 'palmkernel'),\n",
       " (32, 'pet-chem'),\n",
       " (12, 'platinum'),\n",
       " (6, 'potato'),\n",
       " (6, 'propane'),\n",
       " (3, 'rand'),\n",
       " (8, 'rape-oil'),\n",
       " (27, 'rapeseed'),\n",
       " (73, 'reserves'),\n",
       " (25, 'retail'),\n",
       " (59, 'rice'),\n",
       " (49, 'rubber'),\n",
       " (2, 'rye'),\n",
       " (286, 'ship'),\n",
       " (29, 'silver'),\n",
       " (34, 'sorghum'),\n",
       " (26, 'soy-meal'),\n",
       " (25, 'soy-oil'),\n",
       " (111, 'soybean'),\n",
       " (27, 'strategic-metal'),\n",
       " (162, 'sugar'),\n",
       " (2, 'sun-meal'),\n",
       " (7, 'sun-oil'),\n",
       " (16, 'sunseed'),\n",
       " (13, 'tea'),\n",
       " (30, 'tin'),\n",
       " (485, 'trade'),\n",
       " (124, 'veg-oil'),\n",
       " (283, 'wheat'),\n",
       " (29, 'wpi'),\n",
       " (59, 'yen'),\n",
       " (34, 'zinc')]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_per_category(reuters_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categories_per_documents(documents):\n",
    "    \"\"\"Reuters contains multilabeled documents.\n",
    "    This method returns the number of labels and the corresponding number of documents\n",
    "    e.g. 2:1173 means that there are 1173 documents with 2 categories (multilabel)\n",
    "    \"\"\"\n",
    "    def categories_per_document(fid):\n",
    "        return (len(reuters.categories(fid)), fid)\n",
    "\n",
    "\n",
    "    list_of_categories_per_doc = [\n",
    "    categories_per_document(doc) for doc in documents]\n",
    "    # Returns the number of documents that fall in multiple categories\n",
    "    return(Counter([a for (a, b) in list_of_categories_per_doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['coffee',\n",
       "   'copra-cake',\n",
       "   'corn',\n",
       "   'cotton',\n",
       "   'grain',\n",
       "   'palm-oil',\n",
       "   'palmkernel',\n",
       "   'rice',\n",
       "   'rubber',\n",
       "   'soy-meal',\n",
       "   'soybean',\n",
       "   'sugar',\n",
       "   'tea',\n",
       "   'veg-oil',\n",
       "   'wheat'],\n",
       "  'training/235')]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def categories_per_document(fid):\n",
    "        return (len(reuters.categories(fid)), fid)\n",
    "\n",
    "[(reuters.categories(doc),doc) for doc in reuters_documents if categories_per_document(doc)[0] == 15]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few cells set up  the test and train dataset and initialise the tokeniser and model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /Users/Ben/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to /Users/Ben/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7769 3019\n",
      "('AMERICAN STORES &lt;ASC> SEES LOWER YEAR NET\\n  American Stores Co said it\\n  expects to report earnings per share of 3.70 to 3.85 dlrs per\\n  share on sales of slightly over 14 billion dlrs for the year\\n  ended January 31.\\n      The supermarket chain earned 4.11 dlrs per share on sales\\n  of 13.89 billion dlrs last year.\\n      The company did not elaborate.\\n  \\n\\n', [21])\n"
     ]
    }
   ],
   "source": [
    "class Reuters(torch.utils.data.Dataset):\n",
    "    def __init__(self,mode = 'train', tokenise=False):\n",
    "        nltk.download(\"reuters\")\n",
    "        self.fileids = list(filter(lambda doc: doc.startswith(mode), reuters.fileids()))\n",
    "        self.text = [reuters.raw(fid) for fid in self.fileids]\n",
    "        self.category_to_index = {cat:index for (index,cat) in enumerate(reuters.categories())} \n",
    "        self.label = [[self.category_to_index[cat] for cat in reuters.categories(fid)] for fid in self.fileids]\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fileids)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return (self.text[index], self.label[index])\n",
    "   \n",
    "train_dataset = Reuters(\"train\")\n",
    "test_dataset = Reuters(\"test\")\n",
    "\n",
    "print(len(train_dataset), len(test_dataset))\n",
    "print(train_dataset[100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "NUMBER_OF_CLASSES = 90\n",
    "BATCH_SIZE = 5\n",
    "NUMBER_OF_EPOCHS = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokeniser = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUMBER_OF_CLASSES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The commented out line allows us to test the evaluation method runs without running over the whole large dataset by reducing it to a single value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_test = tokeniser(test_dataset.text,truncation=True,padding=True) # these are all default args because of the tokeniser we've loaded with the model we've loaded. \n",
    "tokenised_train = tokeniser(train_dataset.text,truncation=True,padding=True)\n",
    "\n",
    "tokenised_test_dataset = [{\"labels\":nn.functional.one_hot(torch.tensor(label),num_classes=NUMBER_OF_CLASSES).sum(dim=0), \"input_ids\": text,\"attention_mask\":mask} for label,text,mask in zip(test_dataset.label,tokenised_test[\"input_ids\"],tokenised_test[\"attention_mask\"])]\n",
    "tokenised_train_dataset = [{\"labels\":nn.functional.one_hot(torch.tensor(label),num_classes=NUMBER_OF_CLASSES).sum(dim=0), \"input_ids\": text,\"attention_mask\":mask} for label,text,mask in zip(train_dataset.label,tokenised_train[\"input_ids\"],tokenised_train[\"attention_mask\"])]\n",
    "\n",
    "# tokenised_train_dataset = tokenised_train_dataset[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(tokenised_train_dataset, batch_size=BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_position_weights(tokenised_train_dataset):\n",
    "    labels = torch.stack([x[\"labels\"]   for x in tokenised_train_dataset ])\n",
    "    total_positive_samples = labels.sum().item()\n",
    "    total_negative_samples = torch.numel(labels)-total_positive_samples\n",
    "\n",
    "    # print(total_positive_samples,total_negative_samples)\n",
    "    positive_per_label = labels.sum(dim=0)\n",
    "    position_weights = total_negative_samples/positive_per_label\n",
    "    return position_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_update = model.parameters()\n",
    "optimiser = optim.AdamW(params_to_update)\n",
    "position_weights = create_position_weights(tokenised_train_dataset)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=position_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:02<00:11,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss:  0.018846383318305016\n",
      "Epoch Accuracy:  0.9888888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:05<00:08,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss:  0.03621345944702625\n",
      "Epoch Accuracy:  1.9777777777777779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:08<00:05,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss:  0.05208074487745762\n",
      "Epoch Accuracy:  2.966666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:11<00:02,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss:  0.06613552011549473\n",
      "Epoch Accuracy:  3.9555555555555557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:14<00:00,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss:  0.08071557525545359\n",
      "Epoch Accuracy:  4.944444444444445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "running_loss = 0\n",
    "running_correct = 0\n",
    "\n",
    "for epoch in tqdm(range(NUMBER_OF_EPOCHS)):\n",
    "    for batch in train_dataloader: \n",
    "        optimiser.zero_grad()\n",
    "        input_ids = torch.stack(batch[\"input_ids\"], 1)\n",
    "        attention_mask = torch.stack(batch[\"attention_mask\"],1)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "        outputs = model(input_ids,attention_mask= attention_mask)\n",
    "        loss = loss_fn(outputs[\"logits\"], labels.float())\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        running_loss += loss.item() * input_ids.size(0)\n",
    "        preds = (torch.sigmoid(outputs[\"logits\"])>0.5).int()\n",
    "        running_correct += torch.sum(preds==labels)\n",
    "    epoch_loss = running_loss/len(train_dataloader.dataset)\n",
    "    epoch_accuracy = running_correct.double()/(len(train_dataloader.dataset)*NUMBER_OF_CLASSES) \n",
    "    print(\"Epoch loss: \", epoch_loss)\n",
    "    print(\"Epoch Accuracy: \", epoch_accuracy.item())\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PATH = f\"models/reuters_{MODEL_NAME}.pth\"\n",
    "model.load_state_dict(torch.load(MODEL_PATH,map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = 2\n",
    "test_sample = tokenised_test_dataset[test_idx]\n",
    "test_input = torch.tensor(test_sample[\"input_ids\"],device = DEVICE).unsqueeze(0)\n",
    "test_mask = torch.tensor(test_sample[\"attention_mask\"],device = DEVICE).unsqueeze(0)\n",
    "test_label = test_sample[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MASK] japan to revise long - term energy demand downwards the ministry of international trade and industry ( miti ) will revise its long - term energy supply / demand outlook by august to meet a forecast downtrend in japanese energy demand, ministry officials said. miti is expected to lower the projection for primary energy supplies in the year [SEP]'"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input\n",
    "\n",
    "tokeniser.convert_tokens_to_string(tokeniser.convert_ids_to_tokens([103,  2900,  2000,  7065,  5562,  2146,  1011,  2744,  2943,  5157,\n",
    "         28457,  1996,  3757,  1997,  2248,  3119,  1998,  3068,  1006, 10210,\n",
    "          2072,  1007,  2097,  7065,  5562,  2049,  2146,  1011,  2744,  2943,\n",
    "          4425,  1013,  5157, 17680,  2011,  2257,  2000,  3113,  1037, 19939,\n",
    "          2091,  7913,  4859,  1999,  2887,  2943,  5157,  1010,  3757,  4584,\n",
    "          2056,  1012, 10210,  2072,  2003,  3517,  2000,  2896,  1996, 13996,\n",
    "          2005,  3078,  2943,  6067,  1999,  1996,  2095,102]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWARDS\\n  The Ministry of International Trade and\\n  Industry (MITI) will revise its long-term energy supply/demand\\n  outlook by August to meet a forecast downtrend in Japanese\\n  energy demand, ministry officials said.\\n      MITI is expected to lower the projection for primary energy\\n  supplies in the year 2000 to 550 mln kilolitres (kl) from 600\\n  mln, they said.\\n      The decision follows the emergence of structural changes in\\n  Japanese industry following the rise in the value of the yen\\n  and a decline in domestic electric power demand.\\n      MITI is planning to work out a revised energy supply/demand\\n  outlook through deliberations of committee meetings of the\\n  Agency of Natural Resources and Energy, the officials said.\\n      They said MITI will also review the breakdown of energy\\n  supply sources, including oil, nuclear, coal and natural gas.\\n      Nuclear energy provided the bulk of Japan's electric power\\n  in the fiscal year ended March 31, supplying an estimated 27\\n  pct on a kilowatt/hour basis, followed by oil (23 pct) and\\n  liquefied natural gas (21 pct), they noted.\\n  \\n\\n\",\n",
       " [17, 49])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_sample\n",
    "# test_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_idx = 2350\n",
    "test_train_sample = tokenised_train_dataset[test_train_idx]\n",
    "test_train_input = torch.tensor(test_train_sample[\"input_ids\"],device = DEVICE).unsqueeze(0)\n",
    "test_train_mask = torch.tensor(test_train_sample[\"attention_mask\"],device = DEVICE).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BUFFTON CORP &lt;BUFF> BUYS B AND D INSTRUMENTS\\n  Buffton Corp said it completed\\n  the purchase of B and D Industruments Inc for two mln dlrs cash\\n  and 400,000 shares of common stock.\\n      It said B and D is a private company headquartered in\\n  Kansas, and had sales of 4,700,000 dlrs in 1986.\\n      Buffton said the company designs and manufactures aviation\\n  computer display systems and engine instrumentation.\\n  \\n\\n'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.text[2350]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(test_input,attention_mask= test_mask)[\"logits\"]\n",
    "# loss_fn(outputs, labels.float())\n",
    "\n",
    "print(\"score\",label_ranking_average_precision_score(test_label.unsqueeze(0), outputs.detach()))\n",
    "\n",
    "preds = (torch.sigmoid(outputs)>0.5).int()\n",
    "# preds = torch.sigmoid(outputs)\n",
    "print(preds,test_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([90])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_train_mask\n",
    "test_sample\n",
    "test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = 10\n",
    "test_samples = tokenised_test_dataset[:test_idx]\n",
    "test_inputs = torch.stack([torch.tensor(test_sample[\"input_ids\"],device = DEVICE) for test_sample in test_samples])\n",
    "test_masks = torch.stack([torch.tensor(test_sample[\"attention_mask\"],device = DEVICE) for test_sample in test_samples])\n",
    "test_labels = torch.stack([test_sample[\"labels\"] for test_sample in test_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 13675,  2050,  2853, 16319,  2751,  2005,  6146, 19875,  2078,\n",
       "         21469,  2869,  1011,  1059, 14341,  3636,  1004,  8318,  1025,  1059,\n",
       "         14341,  3636, 10495, 17953,  1028,  2056,  1996, 12360,  2009,  2003,\n",
       "          2877,  2097,  3477,  6146,  1012,  4583, 19875,  2078, 21469,  2869,\n",
       "          2005,  1996,  7654,  1997, 13675,  2050,  5183,  1005,  1055,  1004,\n",
       "          8318,  1025, 13675, 11057,  1012,  1055,  1028,  1004,  8318,  1025,\n",
       "         16319,  2751, 13866,  2100,  5183,  1028,  3131,  1010,  2988,  7483,\n",
       "          1012, 13675,  2050,  1998,  1059, 14341,  3636,  2106,  2025, 26056,\n",
       "          1996,  3976,  7483,  1012,  1059, 14341,  3636,  2097,  2907,  4008,\n",
       "          7473,  2102,  1997,  1996, 12360,  1010,  2096,  1004,  8318,  1025,\n",
       "         17151,  2102,  2860, 14341,  4219, 17953,  1028,  2097,  2907,  2676,\n",
       "          7473,  2102,  1998,  1004,  8318,  1025, 13675, 22504,  2271,  5471,\n",
       "         17953,  1028,  2756,  7473,  2102,  1010,  2009,  2056,  1999,  1037,\n",
       "          4861,  1012,  2004,  2988,  1010, 16319,  2751,  8617,  2048,  7134,\n",
       "          1999,  2530,  2660,  5155,  1037,  4117,  4261,  1010,  2199, 19471,\n",
       "          2015,  1997,  2751,  1037,  2095,  1012,  2009,  2036,  8617,  2019,\n",
       "         29341,  2751,  2622,  1012,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 0.16754075701311438\n"
     ]
    }
   ],
   "source": [
    "outputs = model(test_inputs,attention_mask= test_masks)[\"logits\"]\n",
    "# loss_fn(outputs, labels.float())\n",
    "\n",
    "print(\"score\",label_ranking_average_precision_score(test_labels, outputs.detach()))\n",
    "\n",
    "# preds = (torch.sigmoid(outputs)>0.5).int()\n",
    "# # preds = torch.sigmoid(outputs)\n",
    "# print(preds,test_labels)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e147a4062cc14d8132ddbab6c810acf2b57507c0a7010e464731e08cfaff5b7c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
