{"cells":[{"cell_type":"markdown","metadata":{},"source":[" # Finetune the AlexNet Model\n"," Fine-tuning is a process that takes a model that has already been trained for one given task and then tunes or tweaks the model\n"," to make it perform a second similar task"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from __future__ import print_function\n","from __future__ import division\n","from pyexpat import model\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import time\n","import os\n","import copy\n"]},{"cell_type":"markdown","metadata":{},"source":[" # Training Loop\n"," First step of the fine tunning process is to develop a training loop.\n"," We need to understand the following terminology before developing our training loop.\n","\n"," Gradient Descent: It is an optimization algorithm for finding a global minimum of a differentiable function.\n"," Gradient descent is simply used in machine learning to find the values of a function's parameters (coefficients)\n"," that minimize a cost function as far as possible.\n"," We can compute the gradient descent using 4 steps and they are as follows.\n"," Step 1: Compute the loss. Step 2:Compute the gradients. Step 3: update the parameters. Step 4: Rinse and repeat.\n","\n"," Epoch: Refers to one cycle through the full training dataset.\n"," Usually, training a neural network takes more than a few epochs.\n"," In other words, if we feed a neural network the training data for more than one epoch in different patterns, i.e by shuffling data.\n"," we hope for a better generalization when given a new \"unseen\" input (test data).\n","\n"," For each epoch, there are four training steps:\n"," Compute model's prediction, compute the loss, compute the gradients for every parameter and update ther parameters.\n","\n"," Optimizer: An optimizer takes the parameters we want to update, the learning rate we want to use and performs the updates through its step() method.\n"," There are many optimizers available in pytorch and two of which are SGD and Adam.\n","\n"," Loss funtion: It's a method of evaluating how well specific algorithm models the given data.\n"," If predictions deviates too much from actual results, loss function would return a very large number.\n"," Various loss functions available in pytorch and can be referred here: https://pytorch.org/docs/stable/nn.html#loss-functions.\n"," In our code we are using nn.CrossEntropyLoss. This criterion computes the cross entropy loss between input and target."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"]},{"cell_type":"markdown","metadata":{},"source":[" Above code snippet checks for GPU in the system. If the system doesn't have any GPU, then it will perform its training in CPU."]},{"cell_type":"markdown","metadata":{},"source":[" In the below snippet, we are performing both training and validation for all the 50 epochs.\n"," When the model is in training phase, we are calculating the loss from its output and optimizing it.\n"," Also loss and accracy of is calculated for each epoch.\n"," In the validation mode, whenever epoch accuracy is greater than the best accuracy\n"," we are updating it with epoch accuracy and performing a deepcopy on model weights.\n"," And in the end we are returning the updated model with its accuracy history."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, device=\"cpu\"):\n","    since = time.time()\n","    val_acc_history = []\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","            running_loss = 0.0\n","            running_corrects = 0\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    # Get model outputs and calculate loss\n","                    outputs = model(inputs)\n","                    loss = criterion(outputs, labels)\n","                    _, preds = torch.max(outputs, 1)\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","            if phase == 'val':\n","                val_acc_history.append(epoch_acc)\n","        print('')\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model, val_acc_history\n"]},{"cell_type":"markdown","metadata":{},"source":[" # Freezing and Replacing layers\n"," Freezing a layer prevents its weights from being modified and prevents gradients being computed.\n"," This is a trasnfer learning technique, where the base model(trained on some other dataset)is frozen.\n","\n"," This helper function in the below snippet sets the .requires_grad attribute of the parameters in the\n"," model to only want to compute gradients for the newly initialized layer"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def set_parameter_requires_grad(model, only_tune_head):\n","    if only_tune_head:\n","        for param in model.parameters():\n","            param.requires_grad = False"]},{"cell_type":"markdown","metadata":{},"source":[" Adding trainable layers on top of the frozen layers, can make the model learn to turn the old features into predictions on a new dataset.\n"," So, we replaced the 1000 neurons of the pre-trained AlexNet/ResNet model with 10 neurons where each neuron represents a single digit."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def initialize_model(model_name, num_classes, only_tune_head, use_pretrained=True):\n","    # Replace the prediction head of the network to accomadate the new number of classes\n","    model_ft = None\n","    input_size = 0\n","    if model_name == \"resnet\":\n","        \"\"\" Resnet18\n","        \"\"\"\n","        model_ft = models.resnet18(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, only_tune_head)\n","        num_ftrs = model_ft.fc.in_features\n","        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n","        input_size = 224\n","    elif model_name == \"alexnet\":\n","        \"\"\" Alexnet\n","        \"\"\"\n","        model_ft = models.alexnet(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, only_tune_head)\n","        num_ftrs = model_ft.classifier[6].in_features\n","        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n","        input_size = 224\n","    else:\n","        print(\"Invalid model name, exiting...\")\n","        exit()\n","    return model_ft, input_size"]},{"cell_type":"markdown","metadata":{},"source":[" Some common initalization before staring the training has been made in the below snippet."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Models to choose from  are resnet and alexnet\n","MODEL_NAME = \"alexnet\"\n","# Number of classes in the dataset\n","NUM_CLASSES = 10\n","# Batch size for training (change depending on how much memory you have)\n","BATCH_SIZE = 1024\n","# Number of epochs to train for\n","NUM_EPOCHS = 50\n","# Modify finetune. When False, we finetune the whole model,\n","# when True we only update the reshaped layer params\n","ONLY_TUNE_HEAD = True\n","model_ft, input_size = initialize_model(MODEL_NAME, NUM_CLASSES, ONLY_TUNE_HEAD, use_pretrained=True)\n"]},{"cell_type":"markdown","metadata":{},"source":[" # MNIST Data and Data loaders\n"," The MNIST dataset is an acronym that stands for the Modified National Institute of Standards and Technology dataset.\n"," It is a dataset of 60,000 small square 28Ã—28 pixel grayscale images of handwritten single digits between 0 and 9.\n"," Also MNIST data is a greyscal so we stack the image to create 3 channels.\n","\n"," Pytorch Data loader: Combines a dataset and a sampler, and provides an iterable over the given dataset.\n"," The DataLoader supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["transform = transforms.Compose([\n"," transforms.Resize(input_size),\n"," transforms.ToTensor(),\n"," transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n"," transforms.Normalize(\n"," mean=[0.485, 0.456, 0.406],\n"," std=[0.229, 0.224, 0.225]),\n"," ])\n","\n","mnist_train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","mnist_test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","dataloaders_dict = {\n","    \"train\": torch.utils.data.DataLoader(mnist_train_dataset, batch_size=BATCH_SIZE, shuffle=True),\n","    \"val\": torch.utils.data.DataLoader(mnist_test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","}\n"]},{"cell_type":"markdown","metadata":{},"source":[" Before starting our training process. We need to send our model to GPU using the below code snippet.\n"," So that training can be done faster. It is also possible to train the model with CPU but the training time will be high."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Send the model to GPU\n","model_ft = model_ft.to(DEVICE)\n"]},{"cell_type":"markdown","metadata":{},"source":[" Next step is to collect the parameters which needs to be optimized/updated in this run.\n"," Can be gathered using .parameters() function like shown in the below snippet."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["params_to_update = model_ft.parameters()\n"]},{"cell_type":"markdown","metadata":{},"source":[" Setup the optimiser and Loss function:\n"," In this training, we are using one of the popular optimisers called Adam optimiser.\n"," It is easy to implement, will compute efficiently and requires less memory space.\n"," Here, we just need to pass the following parameters(parameters of the layers to update, learning rate, epsilon and weight_decay),\n"," Pytorch's built-in function will take care of the computation.\n","\n"," Similary, instead of we calculation the loss manually\n"," we can use the pytorch's buit-in function to compute the loss between input and target like shown in the below snippet."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Setup the optimiser\n","optimiser_ft = optim.Adam(params_to_update, lr=0.001, eps=1e-08, weight_decay=0.0)\n","# Setup the loss fxn\n","criterion = nn.CrossEntropyLoss()\n","\n"]},{"cell_type":"markdown","metadata":{},"source":[" Train and evaluate:\n"," We have created our own custom function to train and evaluate the model.\n"," model, dataloader dictionary, loss function, optimiser funtion, number of epochs and device typer are the parameters for our custom function.\n"," Once after training, we will have the updated model along with its history in the following model_ft and hist variables respectively."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimiser_ft, num_epochs=NUM_EPOCHS, device=DEVICE)\n"]},{"cell_type":"markdown","metadata":{},"source":[" Save and export the model:\n"," After sucessfully performing our training with MNIST dataset, we need to save our model. So that it can be shared.\n"," To save the model, we are first creating a directly called model and performing the save operation using torch.save()."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["os.makedirs(\"models\", exist_ok=True)\n","torch.save(model_ft.state_dict(), f\"models/mnist_{MODEL_NAME}.pt\")\n","print('Done')\n"]},{"cell_type":"markdown","metadata":{},"source":[" On sucessful completion of the above code model with .pt extension can be found in the models directory.\n"," Later this can be share across team for further testing or developement."]}],"metadata":{"interpreter":{"hash":"1bef641ee2b99dd4e1e56ac47852d0ba29fb20988eed7040973320394f87b8a9"},"kernelspec":{"display_name":"Python 3.9.10 ('.vn': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
