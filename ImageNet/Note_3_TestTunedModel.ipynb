{"cells":[{"cell_type":"markdown","metadata":{},"source":[" # Test the tuned Model\n","\n"," In the last notebook, we have seen the steps to fine tune AlexNet model with MNIST dataset.\n"," In this notebook, we will load the tuned model shared by NICD and then will test it with MNIST data.\n","\n"," In the import section below, we have used two new import which are torch.nn and matplotlib.pyplot.\n"," In which torch.nn provide us many more classes and modules to implement and train the neural network and\n"," matplotlib.pyplot is a collection of functions that make matplotlib work like MATLAB through which we can create figures."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from torchvision import models, transforms, datasets\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n",""]},{"cell_type":"markdown","metadata":{},"source":[" The Transforms module used in the below snippet helped us in transforming the greyscaled MNIST data into a tensor.\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["transform = transforms.Compose([\n"," transforms.Resize(224),\n"," transforms.ToTensor(),\n"," # MNIST is greyscale so we stack the image to create 3 channels\n"," transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n"," transforms.Normalize(\n"," mean=[0.485, 0.456, 0.406],\n"," std=[0.229, 0.224, 0.225]),\n"," ])\n",""]},{"cell_type":"markdown","metadata":{},"source":[" Torchvision provides many built-in datasets in the torchvision.datasets module.\n"," we have downloaded the MNIST datasets here and stored it in the directory named as Data.\n"," Data loader used here combines a dataset and a sampler, and provides an iterable over the given dataset."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mnist_test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","MNIST_test_loader = torch.utils.data.DataLoader(mnist_test_dataset, batch_size=1, shuffle=True)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" In the below snippet, we have created a alexnet model without its pretrained feature.\n"," If we execute the model, we can see the model architecture with out_feature =1000 in its last layer."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = models.alexnet()\n","model"]},{"cell_type":"markdown","metadata":{},"source":[" # Redefining the layers\n"," The task is to classify a given image of a handwritten digit into one of 10 classes representing integer values from 0 to 9, inclusively.\n"," So we need to replace the last layer with 10 neurons.\n"," nn. Linear(n,m) is a module that creates single layer feed forward network with n inputs and m output.\n"," Through which we can update the output of our last layer."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_ftrs = model.classifier[6].in_features\n","model.classifier[6] = nn.Linear(num_ftrs,10)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" If we check the model architecture, we can see the last layer out_features was updated to 10."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model"]},{"cell_type":"markdown","metadata":{},"source":["\n"," Now, we have our expected model architecture for MNIST data pretection.\n"," But it needs to be trained to predict MNIST data. Instead of training the model with MNIST dataset, we directly loaded the finetuned alexnet model head shared by NICD.\n"," Training a model in a CPU machine will take a long time.\n"," So with the help of NICD, we finttuned our pretrained alexnet model with MNIST dataset in NICD's GPU machine which took approximately 40 to 50 mins for 50 epochs.\n","\n"," To load this finetuned model, we need to place the model file (mnist_head_alexnet.pth) in the same directory of our source code.\n"," Then using torch.load() we can deserialize python's object files to memory.\n"," Finally, using load_state_dict, we can loads a model’s parameter dictionary using a deserialized state_dict.\n","\n"," In PyTorch, the learnable parameters (i.e. weights and biases) of an torch.nn.Module model are contained in the model’s parameters (accessed with model.parameters()).\n"," A state_dict is simply a Python dictionary object that maps each layer to its parameter tensor.\n"," Note that only layers with learnable parameters (convolutional layers, linear layers, etc.) and registered buffers have entries in the model’s state_dict."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.load_state_dict(torch.load('mnist_head_alexnet.pth',map_location=torch.device('cpu')))\n","model.eval()\n",""]},{"cell_type":"markdown","metadata":{},"source":[" Since, we are going to test the model directly we are using eval mode in the above snippet.\n"," Next, we need to pass our MNIST data into our model and predict the output.\n"," So, using the below code snippet we iterated and picked a single data from our dataset."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sample_image = iter(MNIST_test_loader).next()[0]\n","sample_image"]},{"cell_type":"markdown","metadata":{},"source":[" Our sample image is in tensor format.\n"," To visualise the selected data, we used matplotlib.pyplot to create the figure of the image."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pil_iamge = transforms.ToPILImage()(sample_image.squeeze(0))\n","plt.imshow(pil_iamge, cmap='gray')\n",""]},{"cell_type":"markdown","metadata":{},"source":[" Finally, we just passed our sample test data into our model and\n"," model precits the probablities of each tensor[0-9]."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred =model(sample_image)\n","pred"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.argmax(pred)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" Finally, using torch.argmax tensor with highest probabilties is returned and that is our expected output."]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}